import { SayFn } from '@slack/bolt';
import { runCommand, runCommandStream } from './command';
import { ReadableStream } from 'node:stream/web';
import { MessageBatcher } from './message-batcher';

// Define a structure for our expected metadata
export interface GitMetadata {
  commitMessage: string;
  prTitle: string;
  prBody: string;
}

export interface AIProviderConfig {
  provider: 'qwen' | 'openai';
  model?: string;
  apiKey?: string;
}

// Base class with common functionality
export abstract class AIProvider {
  // Abstract methods that must be implemented by providers
  protected abstract getCommand(): string;
  protected abstract getCommandArgs(action: 'plan' | 'execute' | 'branch' | 'commit', options?: { model?: string, apiKey?: string }): string[];

  // Common implementation for generating plans
  async generatePlan(prompt: string, cwd: string): Promise<string> {
    const planningPrompt = `Based on the user request, create a detailed implementation plan. Do not execute any commands or modify any files; only output the plan. The user's request is:\n\n${prompt}`;

    const command = this.getCommand();
    const args = this.getCommandArgs('plan');

    const { stdout } = await runCommand(command, args, {
      cwd,
      input: planningPrompt
    });
    return stdout;
  }

  // Common implementation for executing plans with streaming
  async executePlan(plan: string, cwd: string, say: SayFn, threadTs: string): Promise<void> {
    // --- NEW PROMPT ENGINEERING ---
    // Instruct the AI to use a special prefix for all explanatory text.
    const executionPrompt = `
Please execute the following plan. You will run the necessary shell commands yourself.
When you are providing explanations, comments, or reasoning, you MUST prefix each line with the special marker "[REASONING]".
Any line that does not start with this marker will be considered internal command output and will be hidden from the user.

The plan is:
---
${plan}
---
`;

    const command = this.getCommand();
    const args = this.getCommandArgs('execute');

    const subprocess = runCommandStream(command, args, {
      cwd,
      input: executionPrompt,
    });

    // Common stream processing logic
    await this.processStream(subprocess, say, cwd, threadTs, command);
  }

  // Common implementation for generating branch names
  async generateBranchName(prompt: string, cwd: string): Promise<string> {
    const branchPrompt = `Based on the following user request, generate a short, descriptive, git-compliant branch name (kebab-case, max 40 characters). User request: "${prompt}"\n\nOutput only the branch name and nothing else.`;

    const command = this.getCommand();
    const args = this.getCommandArgs('branch');

    const { stdout } = await runCommand(command, args, { cwd, input: branchPrompt });

    const lines = stdout.trim().split('\n').map(l => l.trim()).filter(Boolean);
    const lastLine = lines[lines.length - 1] || 'ai-agent-task';

    return lastLine
      .toLowerCase()
      .replace(/\s+/g, '-')       // spaces -> dashes
      .replace(/[^a-z0-9-]/g, ''); // remove invalid chars
  }

  // Common implementation for generating commit info
  async generateCommitInfo(prompt: string, diff: string, cwd: string): Promise<GitMetadata> {
    const metaPrompt = `
You are an expert software developer. Based on the original user request and the following code changes (git diff), generate the necessary metadata for a pull request.

Original Request:
---
${prompt}
---

Code Changes (diff):
---
${diff}
---

Provide the output in a single, raw JSON object. Do not include any other text, explanations, or markdown formatting. The JSON object should have the following keys:
- "commitMessage": A conventional commit message (e.g., "feat: add user authentication").
- "prTitle": A clear, concise title for the pull request.
- "prBody": A detailed description for the pull request body. Summarize the changes, explain the "why", and reference the original prompt. Use Markdown.
`;

    const command = this.getCommand();
    const args = this.getCommandArgs('commit');

    const { stdout } = await runCommand(command, args, { cwd, input: metaPrompt });

    try {
      // Find the JSON block in the AI's output
      const jsonMatch = stdout.match(/\{[\s\S]*\}/);
      if (!jsonMatch) {
        throw new Error("AI did not return a valid JSON object.");
      }
      return JSON.parse(jsonMatch[0]) as GitMetadata;
    } catch (error) {
      console.error("Failed to parse JSON from AI for git metadata:", error);
      // Fallback to a generic message if JSON parsing fails
      return {
        commitMessage: `feat: AI-driven changes for prompt: ${prompt.slice(0, 50)}...`,
        prTitle: `AI Agent: ${prompt}`,
        prBody: `This PR was automatically generated by the AI agent based on the prompt:\n\n> ${prompt}`,
      };
    }
  }

  // Common stream processing logic
  protected async processStream(subprocess: any, say: SayFn, cwd: string, threadTs: string, providerName: string): Promise<void> {
    const webStream = ReadableStream.from(subprocess.stdout!);
    const reader = webStream.getReader();
    const decoder = new TextDecoder();
    let buffer = '';

    // Create a message batcher for more efficient Slack messaging
    const batcher = new MessageBatcher(this, say, cwd, threadTs, {
      batchTimeMs: 2000, // Check for messages every 2 seconds
      maxBatchSize: 5    // Send up to 5 messages in a batch
    });

    const timeoutMs = 60 * 60 * 1000;
    const timeout = setTimeout(() => {
      console.error(`⏰ ${providerName} execution timed out, killing process.`);
      subprocess.kill('SIGKILL');
      say({ text: `⏰ ${providerName} execution timed out and was killed.`, thread_ts: threadTs });
    }, timeoutMs);

    try {
      while (true) {
        const { done, value } = await reader.read();
        if (done) break;

        const chunk = decoder.decode(value as BufferSource, { stream: true });
        buffer += chunk;

        const lines = buffer.split('\n');
        if (lines.length > 1) {
          const linesToProcess = lines.slice(0, -1);
          buffer = lines[lines.length - 1];

          for (const line of linesToProcess) {
            if (line.startsWith('[REASONING]')) {
              const reasoningText = line.substring('[REASONING]'.length).trim();
              if (reasoningText) {
                batcher.addMessage(`➡️ ${reasoningText}`);
              }
            } else {
              console.log(`[${providerName} Execution]:`, line);
            }
          }
        }
      }

      if (buffer.trim().startsWith('[REASONING]')) {
        const reasoningText = buffer.substring('[REASONING]'.length).trim();
        if (reasoningText) {
          batcher.addMessage(`➡️ ${reasoningText}`);
        }
      } else {
        console.log(`[${providerName} Execution]:`, buffer);
      }

      try {
        await subprocess;
      } catch (error: any) {
        // If the qwen process fails, we catch the error here.
        console.error(`The ${providerName} process exited with an error...`, error);
        batcher.addMessage(`⚠️ The AI process finished with an error...`);
        // We DO NOT re-throw the error. The function will now exit gracefully.
      }
    } finally {
      // Clean up the batcher and send any remaining messages
      await batcher.destroy();
      clearTimeout(timeout);
    }
  }
}

// Qwen provider implementation
class QwenAIProvider extends AIProvider {
  protected getCommand(): string {
    return 'qwen';
  }

  protected getCommandArgs(action: 'plan' | 'execute' | 'branch' | 'commit'): string[] {
    switch (action) {
      case 'plan':
        return [];
      case 'execute':
        return ['-y'];
      case 'branch':
        return [];
      case 'commit':
        return [];
      default:
        return [];
    }
  }
}

// OpenAI provider implementation
class OpenAIProvider extends AIProvider {
  private model: string;
  private apiKey: string;

  constructor(config: AIProviderConfig) {
    super();
    this.model = config.model || 'gpt-4';
    this.apiKey = config.apiKey || process.env.OPENAI_API_KEY || '';

    if (!this.apiKey) {
      throw new Error('OPENAI_API_KEY is required for OpenAI provider');
    }
  }

  protected getCommand(): string {
    return 'openai';
  }

  protected getCommandArgs(action: 'plan' | 'execute' | 'branch' | 'commit'): string[] {
    const baseArgs = ['--model', this.model, '--api-key', this.apiKey];

    switch (action) {
      case 'plan':
        return baseArgs;
      case 'execute':
        return [...baseArgs, '--stream'];
      case 'branch':
        return baseArgs;
      case 'commit':
        return baseArgs;
      default:
        return baseArgs;
    }
  }
}

export function createAIProvider(config: AIProviderConfig): AIProvider {
  switch (config.provider) {
    case 'qwen':
      return new QwenAIProvider();
    case 'openai':
      return new OpenAIProvider(config);
    default:
      throw new Error(`Unsupported AI provider: ${config.provider}`);
  }
}

// For backward compatibility, export a default Qwen provider
export const ai = new QwenAIProvider();